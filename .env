# Use with completions middleware
#
# docker run -it --net host --rm --name openai-responses-middleware \
#   -e OPENAI_BASE_URL_INTERNAL=http://localhost:11434 \
#   -e OPENAI_BASE_URL=http://localhost:8080 \
#   -e OPENAI_API_KEY=1234 \
#   ghcr.io/teabranch/open-responses-server:latest
#
# VITE_APP_OPENAI_BASE_URL=http://localhost:8080

# Use directly with Ollama:
VITE_APP_OPENAI_BASE_URL=http://localhost:1234/v1
VITE_APP_MODEL=gemma3:4b
# VITE_APP_MODEL=deepseek-r1:latest
VITE_APP_OPENAI_API_KEY=1234
